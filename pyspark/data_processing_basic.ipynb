{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:44.410304Z",
     "start_time": "2020-02-29T12:47:43.971759Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F, Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化与配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:48.999562Z",
     "start_time": "2020-02-29T12:47:44.412291Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置集群\n",
    "config = SparkConf()\n",
    "# config.set('spark.dynamicAllocation.maxExecutors', '8')\n",
    "# config.set('spark.driver.memory', '4G')\n",
    "# config.set('spark.executor.memory', '8G')\n",
    "# config.set('spark.executor.cores', '8')\n",
    "# config.set('spark.yarn.executora.memoryOverhead', '4G')\n",
    "# config.set('spark.sql.shuffle.partitions', '500')\n",
    "# config.set('spark.default.parallelism', '500')\n",
    "# config.set('spark.port.maxRetries', '1000')\n",
    "# config.set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "config.set('spark.master','local[4]')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T02:23:05.190594Z",
     "start_time": "2020-02-29T02:23:05.187896Z"
    }
   },
   "source": [
    "## 创建DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字典方式创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:50.752115Z",
     "start_time": "2020-02-29T12:47:49.002198Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    {'user_id': 'A203', 'country': 'India', 'browser': 'Chrome', 'OS': 'WIN', 'age': 33},\n",
    "    {'user_id': 'A201', 'country': 'China', 'browser': 'Safari', 'OS': 'MacOs', 'age': 35},\n",
    "    {'user_id': 'A205', 'country': 'UK', 'browser': 'Mozilla', 'OS': 'Linux', 'age': 25} \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:52.642326Z",
     "start_time": "2020-02-29T12:47:50.755323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+\n",
      "|   OS|age|browser|country|user_id|\n",
      "+-----+---+-------+-------+-------+\n",
      "|  WIN| 33| Chrome|  India|   A203|\n",
      "|MacOs| 35| Safari|  China|   A201|\n",
      "|Linux| 25|Mozilla|     UK|   A205|\n",
      "+-----+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:52.655610Z",
     "start_time": "2020-02-29T12:47:52.644075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OS: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- browser: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 申明列类型创建\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:52.692584Z",
     "start_time": "2020-02-29T12:47:52.657206Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType().add('user_id', StringType(), True).add('country', StringType(), True).add('browser', StringType(), True).add('OS', StringType(), True).add('age', IntegerType(), True)\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    ('A203', 'India', 'Chrome', 'WIN', 33),\n",
    "    ('A201', 'China', 'Safari', 'MacOS', 35),    \n",
    "    ('A205', 'UK', 'Mozilla', 'Linux', 25),        \n",
    "], schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:52.895223Z",
     "start_time": "2020-02-29T12:47:52.694567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-----+---+\n",
      "|user_id|country|browser|   OS|age|\n",
      "+-------+-------+-------+-----+---+\n",
      "|   A203|  India| Chrome|  WIN| 33|\n",
      "|   A201|  China| Safari|MacOS| 35|\n",
      "|   A205|     UK|Mozilla|Linux| 25|\n",
      "+-------+-------+-------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:52.901356Z",
     "start_time": "2020-02-29T12:47:52.897886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- browser: string (nullable = true)\n",
      " |-- OS: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他方式加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:59.293293Z",
     "start_time": "2020-02-29T12:47:52.903225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "+------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从pandas加载数据到spark里\n",
    "salaries_df = spark.createDataFrame(pd.read_csv('salaries_emp_no_less_20000.csv')).cache()\n",
    "employees_df = spark.createDataFrame(pd.read_csv('employees_emp_no_less_20000.csv')).cache()\n",
    "\n",
    "salaries_df.show(5)\n",
    "employees_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 空值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:59.329585Z",
     "start_time": "2020-02-29T12:47:59.295940Z"
    }
   },
   "outputs": [],
   "source": [
    "df_na = spark.createDataFrame([\n",
    "    {'user_id': 'A203', 'country': None, 'browser': 'Chrome', 'OS': 'WIN', 'age': 33},\n",
    "    {'user_id': 'A201', 'country': 'China', 'browser': None, 'OS': 'MacOs', 'age': 35},\n",
    "    {'user_id': 'A205', 'country': 'UK', 'browser': 'Mozilla', 'OS': 'Linux', 'age': 25} \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:47:59.531038Z",
     "start_time": "2020-02-29T12:47:59.331301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+\n",
      "|   OS|age|browser|country|user_id|\n",
      "+-----+---+-------+-------+-------+\n",
      "|  WIN| 33| Chrome|   null|   A203|\n",
      "|MacOs| 35|   null|  China|   A201|\n",
      "|Linux| 25|Mozilla|     UK|   A205|\n",
      "+-----+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:00.998374Z",
     "start_time": "2020-02-29T12:47:59.532761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+-------+-------+-------+\n",
      "|summary|   OS|              age|browser|country|user_id|\n",
      "+-------+-----+-----------------+-------+-------+-------+\n",
      "|  count|    3|                3|      2|      2|      3|\n",
      "|   mean| null|             31.0|   null|   null|   null|\n",
      "| stddev| null|5.291502622129181|   null|   null|   null|\n",
      "|    min|Linux|               25| Chrome|  China|   A201|\n",
      "|    25%| null|               25|   null|   null|   null|\n",
      "|    50%| null|               33|   null|   null|   null|\n",
      "|    75%| null|               35|   null|   null|   null|\n",
      "|    max|  WIN|               35|Mozilla|     UK|   A205|\n",
      "+-------+-----+-----------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据分布\n",
    "df_na.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:01.269137Z",
     "start_time": "2020-02-29T12:48:01.000075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+\n",
      "|   OS|age|browser|country|user_id|\n",
      "+-----+---+-------+-------+-------+\n",
      "|  WIN| 33| Chrome|       |   A203|\n",
      "|MacOs| 35|       |  China|   A201|\n",
      "|Linux| 25|Mozilla|     UK|   A205|\n",
      "+-----+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 空值填充为空字符串''\n",
    "df_na.fillna('').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:01.504039Z",
     "start_time": "2020-02-29T12:48:01.271102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+\n",
      "|   OS|age|browser|country|user_id|\n",
      "+-----+---+-------+-------+-------+\n",
      "|  WIN| 33| Chrome|unknown|   A203|\n",
      "|MacOs| 35|unknown|  China|   A201|\n",
      "|Linux| 25|Mozilla|     UK|   A205|\n",
      "+-----+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按列填充空值\n",
    "df_na.fillna({'browser': 'unknown', 'country': 'unknown'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:01.692581Z",
     "start_time": "2020-02-29T12:48:01.505806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+\n",
      "|   OS|age|browser|country|user_id|\n",
      "+-----+---+-------+-------+-------+\n",
      "|Linux| 25|Mozilla|     UK|   A205|\n",
      "+-----+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 删除为空的数据\n",
    "df_na.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:01.865233Z",
     "start_time": "2020-02-29T12:48:01.694515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+\n",
      "|   OS|age|browser|country|user_id|\n",
      "+-----+---+-------+-------+-------+\n",
      "|  WIN| 33| Chrome|   null|   A203|\n",
      "|Linux| 25|Mozilla|     UK|   A205|\n",
      "+-----+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 删除某列为空的数据\n",
    "df_na.na.drop(subset='browser').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:02.021403Z",
     "start_time": "2020-02-29T12:48:01.867199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+\n",
      "|age|browser|country|user_id|\n",
      "+---+-------+-------+-------+\n",
      "| 33| Chrome|   null|   A203|\n",
      "| 35|   null|  China|   A201|\n",
      "| 25|Mozilla|     UK|   A205|\n",
      "+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 删除某列\n",
    "df_na.drop('os').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T03:17:17.751233Z",
     "start_time": "2020-02-29T03:17:17.748498Z"
    }
   },
   "source": [
    "## 类SQL操作\n",
    "- Select\n",
    "- Filter\n",
    "- Where\n",
    "- Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案一: 求职工最大工资"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:03.773852Z",
     "start_time": "2020-02-29T12:48:02.023239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+-------------+----------+-------------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender_custom|max_salary|salary_change_times|\n",
      "+------+----------+----------+---------+-------------+----------+-------------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|         male|     88958|                 17|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|       female|     72527|                  6|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|         male|     43699|                  7|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|         male|     74057|                 16|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|         male|     94692|                 13|\n",
      "+------+----------+----------+---------+-------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用group by的方式\n",
    "employees_df.join(\n",
    "    salaries_df\n",
    "    ,salaries_df['emp_no']==employees_df['emp_no']\n",
    ").withColumn(\n",
    "    'gender_custom'\n",
    "    ,F.when(employees_df['gender']=='M', 'male'\n",
    "    ).otherwise('female')\n",
    ").groupBy(\n",
    "    employees_df['emp_no']\n",
    "    ,employees_df['birth_date']    \n",
    "    ,employees_df['first_name']    \n",
    "    ,employees_df['last_name']\n",
    "    ,'gender_custom'\n",
    ").agg(\n",
    "    F.max('salary').alias('max_salary')\n",
    "    ,F.count('salary').alias('salary_change_times')\n",
    ").select(\n",
    "    employees_df['emp_no']\n",
    "    ,employees_df['birth_date']\n",
    "    ,employees_df['first_name']\n",
    "    ,employees_df['last_name']\n",
    "    ,'gender_custom'   \n",
    "    ,'max_salary'\n",
    "    ,'salary_change_times'     \n",
    ").where(\n",
    "    (employees_df['emp_no']>=10001) & (employees_df['emp_no']<=10005)\n",
    ").orderBy(\n",
    "    'emp_no'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二: 求职工最大工资"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:48:04.827807Z",
     "start_time": "2020-02-29T12:48:03.775948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+-------------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender_custom|max_salary|\n",
      "+------+----------+----------+---------+-------------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|         male|     88958|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|       female|     72527|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|         male|     43699|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|         male|     74057|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|         male|     94692|\n",
      "+------+----------+----------+---------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用窗口函数 TopN\n",
    "employees_df.join(\n",
    "    salaries_df\n",
    "    ,salaries_df['emp_no']==employees_df['emp_no']\n",
    ").withColumn(\n",
    "    'gender_custom'\n",
    "    ,F.when(employees_df['gender']=='M', 'male'\n",
    "    ).otherwise('female')\n",
    ").withColumn(\n",
    "    'index'\n",
    "    ,F.row_number().over(Window.partitionBy(employees_df['emp_no']).orderBy(salaries_df['salary'].desc()))\n",
    ").filter(\n",
    "    F.col('index')==1\n",
    ").select(\n",
    "    employees_df['emp_no']\n",
    "    ,employees_df['birth_date']\n",
    "    ,employees_df['first_name']\n",
    "    ,employees_df['last_name']\n",
    "    ,F.col('gender_custom')\n",
    "    ,F.col('salary').alias('max_salary')\n",
    ").where(\n",
    "    (employees_df['emp_no']>=10001) & (employees_df['emp_no']<=10005)\n",
    ").orderBy(\n",
    "    'emp_no'\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emr_py36",
   "language": "python",
   "name": "emr_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
